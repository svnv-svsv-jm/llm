{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bu6pcJsdqbJa"
      },
      "source": [
        "# Conversational RAG agent\n",
        "\n",
        "## How to use\n",
        "\n",
        "Upload your documents (see on the left) and use your HuggingFace token in the secrets. The variable name must be `HF_TOKEN`.\n",
        "\n",
        "Then run everything, wait and go to the bottom: start chatting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0owPW4SnjPH",
        "outputId": "f24cf36b-a1e8-44f0-d971-624c0dc46700"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.82)] [Connecting to security.ubuntu.com (91.189.91.8\r                                                                                                    \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)]\r                                                                                                    \rHit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)]\r                                                                                                    \rHit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\r0% [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)] [Connecting to ppa.la\r0% [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpadcontent.net (185.125.190.\r                                                                                                    \rHit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Ign:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.5).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y tesseract-ocr poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85ROYEjXvnEl"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet --upgrade \\\n",
        "  bitsandbytes \\\n",
        "  torch \\\n",
        "  transformers \\\n",
        "  langchain \\\n",
        "  langchain_community \\\n",
        "  langchain_huggingface \\\n",
        "  \"unstructured[pdf]\" \\\n",
        "  sentence_transformers \\\n",
        "  faiss-gpu \\\n",
        "  pdf2image \\\n",
        "  pytesseract \\\n",
        "  langgraph \\\n",
        "  nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4udG6xvNrQxs",
        "outputId": "f295465d-e1ff-47d0-ed43-6186ce789608"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import nltk\n",
        "\n",
        "# Set NLTK data path\n",
        "os.environ['NLTK_DATA'] = '/root/nltk_data'\n",
        "\n",
        "# Download the 'punkt' tokenizer data\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L11suo2DvmjW"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "79ec37ca6c654742bf5b65ba83b472cc",
            "f681b20cd1ea498dbf529d0047a399e3",
            "574b9749df664515a38b77f6fdf9bbed",
            "a83a364e7dcc4c6aa49d11ae79cde8a8",
            "ce6b11f0b01247c4b15285a4e1ff4abf",
            "61af27c8f098480599e2c61b8e532960",
            "3a129eeb7ae24cd2badd6454b85042a9",
            "7cf5217574d1447dab2b38639fbd7345",
            "a47fabcb27784845ac5bc46f59ee6212",
            "a99782f0536140d5a59baf0f46b33fd4",
            "b877eecb3ee34c33b3d382219a3b2a8b"
          ]
        },
        "id": "MmWJJCqFkVl0",
        "outputId": "1872312a-97b1-4c3a-c928-cb6c414f416e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79ec37ca6c654742bf5b65ba83b472cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_huggingface.llms.huggingface_pipeline:Setting the `device` argument to None from -1 to avoid the error caused by attempting to move the model that was already loaded on the GPU using the Accelerate module to the same or another device.\n"
          ]
        }
      ],
      "source": [
        "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs=dict(\n",
        "        max_new_tokens=512,\n",
        "        do_sample=False,\n",
        "        repetition_penalty=1.03,\n",
        "        return_full_text=False,\n",
        "    ),\n",
        "    model_kwargs={\"quantization_config\": quantization_config},\n",
        ")\n",
        "\n",
        "chat_model = ChatHuggingFace(llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CB9irS82kzyc"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import (\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You're a helpful assistant\"),\n",
        "    HumanMessage(\n",
        "        content=\"What happens when an unstoppable force meets an immovable object?\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "ai_msg = chat_model.invoke(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNnWayaslPya",
        "outputId": "a8899bf4-5736-4349-d537-a5a409abe9c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='According to a popular philosophical paradox, when an unstoppable force meets an immovable object, it is impossible to determine which will prevail because both are defined as being incapable of being stopped or moved, respectively. This paradox raises questions about the nature of force and motion and challenges our understanding of cause and effect. However, in reality, such a scenario is hypothetical and cannot occur in the physical world as both concepts are theoretical extremes that cannot exist simultaneously in the real world.', additional_kwargs={}, response_metadata={}, id='run-4c02c1f2-8a0f-4c74-9695-6c708960c75a-0')"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ai_msg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0Ki1rSTp-Cg",
        "outputId": "035d2f07-db06-4fe6-de6b-dcdbb4f8fe4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "czech.pickle\t finnish.pickle  malayalam.pickle   PY3_tab\t    swedish.pickle\n",
            "danish.pickle\t french.pickle\t norwegian.pickle   README\t    turkish.pickle\n",
            "dutch.pickle\t german.pickle\t polish.pickle\t    russian.pickle\n",
            "english.pickle\t greek.pickle\t portuguese.pickle  slovene.pickle\n",
            "estonian.pickle  italian.pickle  PY3\t\t    spanish.pickle\n"
          ]
        }
      ],
      "source": [
        "!ls /root/nltk_data/tokenizers/punkt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dO4xtFi2rmgw"
      },
      "outputs": [],
      "source": [
        "!cp -R /root/nltk_data/tokenizers/punkt/PY3 /root/nltk_data/tokenizers/punkt/PY3_tab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA8ttVv2l2Ml"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "import typing as ty\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.document_loaders.directory import DirectoryLoader\n",
        "\n",
        "loader = DirectoryLoader(\n",
        "    path=os.path.join('.'),\n",
        "    glob=\"*.pdf\",\n",
        "    recursive=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzkJwV7Gmdlb"
      },
      "outputs": [],
      "source": [
        "docs: ty.List[Document] = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uitV4XnRmkiL"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=30)\n",
        "chunked_docs = splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqfX8j7DmqLA"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.faiss import FAISS\n",
        "\n",
        "# For all model names, see: https://www.sbert.net/docs/pretrained_models.html\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "db = FAISS.from_documents(chunked_docs, embedding=embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sGWz_MMms53"
      },
      "outputs": [],
      "source": [
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSX-cNL80Owl"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "contextualize_q_system_prompt = (\n",
        "    \"Given a chat history and the latest user question \"\n",
        "    \"which might reference context in the chat history, \"\n",
        "    \"formulate a standalone question which can be understood \"\n",
        "    \"without the chat history. Do NOT answer the question, \"\n",
        "    \"just reformulate it if needed and otherwise return it as is.\"\n",
        ")\n",
        "\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm, retriever, contextualize_q_prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pTOokNkocX7"
      },
      "outputs": [],
      "source": [
        "from langchain.tools.retriever import create_retriever_tool\n",
        "\n",
        "# Build retriever tool\n",
        "tool = create_retriever_tool(\n",
        "    history_aware_retriever,\n",
        "    name=\"document_retriever\",\n",
        "    description=\"Searches and returns excerpts from the local database of documents.\",\n",
        ")\n",
        "tools = [tool]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eboqOpZWpDF4"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "memory = MemorySaver()\n",
        "agent_executor = create_react_agent(chat_model, tools, checkpointer=memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CkeKbgUpFyy",
        "outputId": "65e0dab5-6802-437c-aaf6-c6650ad49182"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is Task Decomposition?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Task decomposition is a process in project management and systems engineering that involves breaking down a complex task or project into smaller, more manageable subtasks or components. This approach helps to clarify the scope of the project, identify dependencies between tasks, and allocate resources more effectively. By decomposing tasks into smaller pieces, it becomes easier to estimate time, cost, and resource requirements, as well as to monitor progress and identify potential issues or risks. Task decomposition is an essential part of project planning and execution, as it enables teams to develop detailed work plans, schedules, and budgets, and to ensure that all necessary steps are taken to achieve the desired outcome.\n"
          ]
        }
      ],
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
        "\n",
        "for event in agent_executor.stream(\n",
        "    {\"messages\": [HumanMessage(content=\"What is Task Decomposition?\")]},\n",
        "    config=config,\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6VExTAOpT68",
        "outputId": "882afa00-8634-4c7d-8795-4ff44b3e8b73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What are common ways of doing it?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "There are several common ways to decompose tasks, depending on the nature of the project and the preferences of the project manager or systems engineer. Here are some common approaches:\n",
            "\n",
            "1. Top-down decomposition: This is a hierarchical approach where the project is broken down into larger, more general tasks, which are then further decomposed into smaller, more specific tasks. This approach helps to ensure that all critical aspects of the project are identified and addressed.\n",
            "\n",
            "2. Bottom-up decomposition: This is a more detailed approach where the project is broken down into its smallest possible components, and then these components are grouped together to form larger tasks. This approach helps to ensure that all necessary details are captured and that the project is executed with a high degree of precision.\n",
            "\n",
            "3. Cross-functional decomposition: This approach involves breaking down tasks across multiple functional areas or disciplines, such as engineering, operations, and finance. This approach helps to ensure that all relevant stakeholders are involved in the project and that all necessary perspectives are considered.\n",
            "\n",
            "4. Iterative decomposition: This approach involves breaking down tasks into smaller, more iterative components, which are then refined and improved over time. This approach helps to ensure that the project is flexible and adaptable to changing requirements and circumstances.\n",
            "\n",
            "5. Concurrent decomposition: This approach involves breaking down tasks into parallel components, which can be executed simultaneously. This approach helps to reduce overall project duration and improve resource utilization.\n",
            "\n",
            "The choice of decomposition method will depend on the specific needs of the project and the preferences of the project manager or systems engineer. It's important to select a method that is appropriate for the project and that aligns with the organization's overall project management philosophy.\n"
          ]
        }
      ],
      "source": [
        "query = \"What are common ways of doing it?\"\n",
        "\n",
        "for event in agent_executor.stream(\n",
        "    {\"messages\": [HumanMessage(content=query)]},\n",
        "    config=config,\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfiTUqQnz4OW",
        "outputId": "42ac7e7a-d302-4c8d-8832-6715206fbebd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='What is Task Decomposition?', additional_kwargs={}, response_metadata={}, id='9d7fbf5c-fe3e-49ad-a037-64fa90dfcee3'),\n",
              "  AIMessage(content='Task decomposition is a process in project management and systems engineering that involves breaking down a complex task or project into smaller, more manageable subtasks or components. This approach helps to clarify the scope of the project, identify dependencies between tasks, and allocate resources more effectively. By decomposing tasks into smaller pieces, it becomes easier to estimate time, cost, and resource requirements, as well as to monitor progress and identify potential issues or risks. Task decomposition is an essential part of project planning and execution, as it enables teams to develop detailed work plans, schedules, and budgets, and to ensure that all necessary steps are taken to achieve the desired outcome.', additional_kwargs={}, response_metadata={}, id='run-293a1781-b943-4568-b2ff-417efc1aee45-0'),\n",
              "  HumanMessage(content='What are common ways of doing it?', additional_kwargs={}, response_metadata={}, id='8a79794a-cedc-4bfa-b0ce-18394bd8f4dd'),\n",
              "  AIMessage(content=\"There are several common ways to decompose tasks, depending on the nature of the project and the preferences of the project manager or systems engineer. Here are some common approaches:\\n\\n1. Top-down decomposition: This is a hierarchical approach where the project is broken down into larger, more general tasks, which are then further decomposed into smaller, more specific tasks. This approach helps to ensure that all critical aspects of the project are identified and addressed.\\n\\n2. Bottom-up decomposition: This is a more detailed approach where the project is broken down into its smallest possible components, and then these components are grouped together to form larger tasks. This approach helps to ensure that all necessary details are captured and that the project is executed with a high degree of precision.\\n\\n3. Cross-functional decomposition: This approach involves breaking down tasks across multiple functional areas or disciplines, such as engineering, operations, and finance. This approach helps to ensure that all relevant stakeholders are involved in the project and that all necessary perspectives are considered.\\n\\n4. Iterative decomposition: This approach involves breaking down tasks into smaller, more iterative components, which are then refined and improved over time. This approach helps to ensure that the project is flexible and adaptable to changing requirements and circumstances.\\n\\n5. Concurrent decomposition: This approach involves breaking down tasks into parallel components, which can be executed simultaneously. This approach helps to reduce overall project duration and improve resource utilization.\\n\\nThe choice of decomposition method will depend on the specific needs of the project and the preferences of the project manager or systems engineer. It's important to select a method that is appropriate for the project and that aligns with the organization's overall project management philosophy.\", additional_kwargs={}, response_metadata={}, id='run-ad622e0f-4c1c-4d04-95b3-07dee65ad1b0-0')]}"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "event"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpUUkWCAoxbr"
      },
      "source": [
        "## Chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EELg8SxKoeEs",
        "outputId": "51ae47d2-27e1-4a11-8747-f890cf42da8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask your question: is today a good day for you?\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "is today a good day for you?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "As an artificial intelligence language model, I don't experience days or have personal feelings. I'm always available to assist you with any questions or requests you may have, regardless of the time or day. So, you can feel free to reach out to me anytime you need help.\n",
            "Ask your question: what did i just ask?\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "what did i just ask?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I'm not privy to your previous conversations or thoughts. Please provide more context so I can assist you better. If you're asking this question because you've forgotten what you asked me earlier, please let me know what you remember about the conversation, and I'll do my best to help you recall what was discussed.\n",
            "Ask your question: did i ask about the day?\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "did i ask about the day?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I'm not aware of your previous conversations or messages. However, based on our previous interactions, I can tell you that we've discussed various topics, but I don't recall if you asked about the day specifically. If you're asking this question because you're wondering whether today is a good day, I can assure you that I'm always here to help you, regardless of the time or day. So, please don't hesitate to reach out to me anytime you need assistance.\n",
            "Ask your question: please rewrite the latest question i asked (before this current one, of course)\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "please rewrite the latest question i asked (before this current one, of course)\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Certainly! Please provide the question you asked before your current one, and I'll rephrase it for you.\n",
            "Ask your question: remember it\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "remember it\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I'm sorry, but I'm not capable of remembering anything beyond our current interaction. My responses are generated based on the information you provide, and I don't have the ability to remember previous conversations or messages. However, I'm always here to assist you with any questions or requests you may have, and I'll do my best to provide helpful and accurate responses.\n",
            "Ask your question: i asked you in this very conversation!\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "i asked you in this very conversation!\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I'm sorry if my previous response was unclear. When you asked me to \"remember it,\" I understood that you were asking me to recall a specific question or request from our previous interactions. However, since we've only been communicating in this current conversation, I'm not able to remember any previous questions or requests. I'm always here to help you with any questions or requests you may have, and I'll do my best to provide helpful and accurate responses. Is there a specific question or request you're referring to that I can assist you with? Please let me know, and I'll do my best to help.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-6fe63248a5d8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ask your question: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"stop\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   for event in agent_executor.stream(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "  query = input(\"Ask your question: \")\n",
        "  if query.lower().strip() == \"stop\":\n",
        "    break\n",
        "  for event in agent_executor.stream(\n",
        "      {\"messages\": [HumanMessage(content=query)]},\n",
        "      config=config,\n",
        "      stream_mode=\"values\",\n",
        "  ):\n",
        "      event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iIdLpvWqVso"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3a129eeb7ae24cd2badd6454b85042a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "574b9749df664515a38b77f6fdf9bbed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cf5217574d1447dab2b38639fbd7345",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a47fabcb27784845ac5bc46f59ee6212",
            "value": 8
          }
        },
        "61af27c8f098480599e2c61b8e532960": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79ec37ca6c654742bf5b65ba83b472cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f681b20cd1ea498dbf529d0047a399e3",
              "IPY_MODEL_574b9749df664515a38b77f6fdf9bbed",
              "IPY_MODEL_a83a364e7dcc4c6aa49d11ae79cde8a8"
            ],
            "layout": "IPY_MODEL_ce6b11f0b01247c4b15285a4e1ff4abf"
          }
        },
        "7cf5217574d1447dab2b38639fbd7345": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a47fabcb27784845ac5bc46f59ee6212": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a83a364e7dcc4c6aa49d11ae79cde8a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a99782f0536140d5a59baf0f46b33fd4",
            "placeholder": "​",
            "style": "IPY_MODEL_b877eecb3ee34c33b3d382219a3b2a8b",
            "value": " 8/8 [01:20&lt;00:00,  8.44s/it]"
          }
        },
        "a99782f0536140d5a59baf0f46b33fd4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b877eecb3ee34c33b3d382219a3b2a8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce6b11f0b01247c4b15285a4e1ff4abf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f681b20cd1ea498dbf529d0047a399e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61af27c8f098480599e2c61b8e532960",
            "placeholder": "​",
            "style": "IPY_MODEL_3a129eeb7ae24cd2badd6454b85042a9",
            "value": "Loading checkpoint shards: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
